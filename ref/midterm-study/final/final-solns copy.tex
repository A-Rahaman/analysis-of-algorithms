%Midterm exam for CS461 for Fall 2003

\documentclass[11pt]{article}
\usepackage{fullpage,pst-all,epsfig}

\newcommand{\comment}[1]{}
\newcommand{\Le}{\textbf{L}}

%\newcommand{\ans}[1]{\emph{Solution: #1}}
\newcommand{\ans}[1]{ }

\newcommand{\seq}[1]{ \langle #1,\cdots \, \rangle}
\newcommand{\seqi}[1]{ \langle #1 \rangle}
	
\begin{document}
\thispagestyle{empty}
\begin{center}
\def\handout{Final Examination}
\vspace*{-.75in}
{\large University of New Mexico}\\
{\large Department of Computer Science}\\
\vspace*{0.5in}
{\LARGE {\bf \handout}}\\
\vspace*{0.1in}
{\large CS 561 Data Structures and Algorithms}\\
{\large Fall, 2010}\\ [0.3in]
\end{center}

\vfill

\makeatletter
\long\def\hint#1{({\em Hint\/}: #1)}
% \def\@oddhead{\rm\makebox[0in][l]{CS 461 Midterm ---Fall,
% 2003}\hfil\thepage\hfil\makebox[0in][r]{Name:\rule[-0.1in]{2in}{.5pt}}}
\let\@evenhead\@oddhead
\def\@oddfoot{}
\let\@evenfoot\@oddfoot

\def\problem#1{\def\problemheading{#1}\clearpage\item{\bf #1}}

% Comment out the above 'problem' def and use the one below to get
% all the problems on a single page, instead of page break each time.
%\def\problem#1{\def\problemheading{#1}\item{\bf #1}}

\def\extrapage{\addtocounter{enumi}{-1}\clearpage\item{\bf \problemheading, continued.}}

\let\part\item
\renewcommand{\theenumii}{\alph{enumii}}
\makeatother
\parindent 0pt

\vfill
\centerline{
\Large
\begin{tabular}{|l|}  \hline
Name: \hspace*{2in} \\ \hline
Email: \hspace*{2in}\\ \hline
\end{tabular}
}
\vfill

\hrule
\begin{itemize}

\item ``Nothing is true.  All is permitted'' - Friedrich Nietzsche.  Well, not exactly.  \emph{You are not permitted to discuss this exam with any other person.}  If you do so, you will surely be smitten.  You may consult any \emph{other} sources including books, papers, web pages, computational devices, animal entrails, seraphim, cherubim, etc. in your quest for truth and solutions.  Please acknowledge your sources.

\item {\em Show your work!}  You will not get full credit if we cannot figure out how you arrived at your answer.  A numerical solution obtained via a computer program is unlikely to get much credit, if any, without a correct mathematical derivation.

\item Write your solution in the space provided for the corresponding problem.

\item If any question is unclear, ask for clarification.

\end{itemize}
\hrule
\vfill
\centerline{
\Large
\begin{tabular}{|c|c|c|c|}  \hline
Question & Points & Score & Grader \\  \hline\hline
1 & 25 & & \\  \hline
2 & 25 & & \\  \hline
3 & 20 & & \\  \hline
4 & 25 & & \\  \hline
5 & 25 & & \\  \hline
\hline Total & 120 & & \\  \hline
\end{tabular}
}
\vfill

\newpage

\begin{enumerate}
 
 
 \problem{Spanning Trees and Amortizations}
 
 \begin{enumerate}
 \item (7 points) Prove that in a graph where all edge weights are the same, that any shortest path tree will also be a minimum spanning tree.
 
\ans{In such a graph, any spanning tree is also a MST since any spanning tree contains exactly $n-1$ edges} \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\
  
\pagebreak
 
 \item (7 points) Professor Plum postulates that if every edge on an undirected graph has a unique positive weight, then the shortest path tree rooted at $v$ on that graph is always the same as the minimum spanning tree found by Prim's algorithm when seeded initially with the vertex $v$.  Is this correct?  If so, prove it.  If not, give a counter example.

\ans{No.  Consider the graph over vertices $v,w,x$ where edge $(v,w)$ has weight $1$, edge $(v,x)$ has weight $2.5$ and edge $(w,x)$ has weight $2$.  The shortest path tree for $v$ is edges $(v,w)$ and $(v,x)$.  The MST found by Prim's when started at $v$ is $(v,x)$ and $(w,x)$.}

\pagebreak

Consider a data structure over an initially empty list that supports the following two operations.  APPEND-NUMBER(x): Adds the number $x$ to the beginning of the list; and MIN-MAX: Traverses the list, computing the minimum and maximum element in the list, and then creates a new list that contains only two numbers, the minimum and maximum of the old list. 

\begin{enumerate}
\item (4 points) Assume an arbitrary sequence of $n$ operations are performed on this data structure.  What is the worst case run time of any particular operation? \ans{$\Theta(n)$.  First $n-1$ APPEND-NUMBER operations and then a MIN-MAX operation.}
\ \\ \ \\ \ \\ \ \\
\item (7 points) Show that the amortized cost of an operation is $O(1)$ using the potential method.  Make sure to prove your potential function is valid.
\ans{Let $\phi(D)$ equal the number of items in the list.  This is a valid potential function (Why?).  The amortized cost of an APPEND-NUMBER operation is then $c_{i} + \phi_{i} - \phi_{i-1} = 2$.  The amortized cost of a MIN-MAX operation is $l_{i}+(2-l_{i}) = 2$ where $l_{i}$ is the length of the list at time $i$.}
\end{enumerate}
 
\end{enumerate}

 
 \problem{Shortest Paths, Commodities and the Moral Torpidity of American Enterprise}
 
 \emph{``A million dollars isn't cool.  You know what's cool?  A billion dollars'' - Sean Parker in The Social Network}
 
What if we change the all-pairs, shortest paths problem so there is some cost associated with each intermediate vertex that is traversed in a path?  In particular, we are given a graph $G$ that has weights, $w$, on all the edges and also weights, $w'$, on all the vertices.  The weight of a path is the sum of the weights of all edges in that path and the weights of all vertices in the path except for the start and end vertex.  For example, the weight of the path $v_{1}, v_{2}, v_{3}, v_{4}$ is $w(v_{1} \rightarrow v_{2}) + w(v_{2} \rightarrow v_{3}) + w(v_{3} \rightarrow v_{4}) + w'(2) + w'(3)$.  The weights of both edges and vertices may be negative.  Your first goal is to design an algorithm that either computes the weight of the shortest path between every pair of vertices if there is no negative weight cycle in the graph, or else if there is a negative weight cycle, correctly returns that this is the case.

You will show how to modify the Floyd Warshall algorithm to solve this new variant of all pairs shortest paths.  As in lecture, let $dist(u,v,r)$ be the shortest path from $u$ to $v$ that traverses vertices numbered $r$ or less.  Also, assume that the vertices are uniquely labelled from $1$ to $n$

\begin{enumerate}
\item (8 points) Write down a recurrence relation for $dist(u,v,r)$ for this new variant of the shortest paths problem. 

\ans{$dist(u,v,r) = w(u \rightarrow v)$ if $r=0$\\ else $dist(u,v,r) = min (dist(u,v,r-1), dist(u,r,r-1) + dist(r,v,r-1) + w'(r)$)}  \ \\ \ \\ \ \\ 

\pagebreak

\item (7 points) Write down an algorithm, based on Floyd Warshall that solves this variant of the shortest path problem.  Remember your algorithm should return that there is a negative weight cycle if one exists.

\ans{Run the lecture notes Floyd Warshall with the above recurrence.  Then at the end, check $dist(u,u,n)$ for all nodes $u$, if any of these values are negative, output that there is a negative weight cycle.}


\pagebreak

\item (10 points) Now imagine that you're trying to make money in the following variant of the arbitrage problem.  There are $n$ commodities $g_{1}, g_{2}, \ldots, g_{n}$ and an n by n table $R$ of exchange rates, such that one one unit of commodity $g_{i}$ buys $R[i,j]$ units of commodity $j$.  Moreover, there are \emph{taxes} on each commodity given by $t_{1}, t_{2}, ..., t_{n}$ such that if $i$ is an intermediate commodity in a sequence of trades, you are taxed at rate $t_{i}$ when you convert to commodity $i$.  For example, when you convert to the intermediate commodity ``pork bellies'', you are taxed at a rate of $.05$.  A \emph{valid} sequence of trades, starts and ends with the same commodity and contains every other commodity at most once.   The \emph{profit} from a sequence of trades is the amount of the first commodity you are left with if you start with one unit of the first commodity initially and perform the trades in the sequence.  For example, if there is a sequence of commodities $g_{i_{1}}, g_{i_{2}}, \ldots, g_{i_{k}}, g_{i_{1}}$, then the profit for that sequence is\\ $R[i_{1},i_{2}]\cdot (1 - t_{2}) \cdot R[i_{2}, i_{3}] \cdot(1 -  t_{3}) \cdots R[i_{k-1},i_{k}] \cdot (1- t_{k}) \cdot R[i_{k},i_{1}]$

(Note that you are never taxed for the start/end commodity in the valid sequence, and that the profit may be less than $1$.)

Design an algorithm that outputs the maximum profit from a valid sequence of trades.

\ans{Create a graph with a vertex for every commodity.  Let the weight of edge $u \rightarrow v$ equal $- \log R[u,v]$.  Let the weight of vertex $v$ equal $- \log (1-t_{v})$.  Now run modified Floyd Warshall with the recurrence in the previous problem to fill in all entries of the array.  Find the vertex $v$ that minimizes $dist(v,v,n)$; return $2^{-dist(v,v,n)}$}


\end{enumerate}



 \problem{Linear Programming}

You are taking a penalty shot in a soccer game and you must decide whether to kick the ball left or right.  At the same time, your opponent, the goalie, must decide whether to dive left or to dive right.  Through careful study of your skills and the goalie's skills, you have determined the probabilities of a goal in all cases: $p_{\ell,\ell}$ when you both go left, $p_{r,r}$ when you both go right, $p_{\ell,r}$ when you kick left and your opponent dives right, and $p_{r,\ell}$ when you kick right and your opponent dives left.  Note that these probabilities are not necessarily symmetric because of right/left handed/footed-ness.

Your goal is to to determine a probability distribution over your $2$ choices that will maximizes your probability of a goal,  \emph{given that your opponent knows your strategy and plays optimally for that strategy}!  In particular, you want to compute optimal probabilities $q_{\ell}$ of kicking left and $1-q_{\ell}$ of kicking right.

\begin{enumerate}

\item (10 points) Write down a linear program to find your optimal strategy and the probability of getting a goal from that strategy if your opponent plays optimally.  Hint 1: If your opponent knows your strategy, her own strategy will either be to dive left with probability $1$, or dive right with probability $1$.   Hint 2: Use a trick similar to the one that we used in designing the linear program for the shortest paths problem.  
\ans{
Maximize w\\
Subject to:\\
$$ 0 \leq q_{\ell} \leq 1$$
$$ w \leq q_{\ell}p_{\ell,\ell} + (1-q_{\ell})p_{r,\ell}$$ %dive left
$$ w \leq q_{\ell}p_{\ell,r} + (1-q_{\ell})p_{r,r}$$ %dive right
}

\pagebreak

Now consider a generalization of the problem where the goal is divided into $n$ regions.  You must decide on one of the $n$ possible regions to kick the ball and the goalie must decide on one of $n$ possible regions to dive to.  You now have for all $1 \leq i,j \leq n$, probability $p_{i,j}$ which is the probability of scoring a goal if you kick to region $i$ and the goalie dives to region $j$.

\item (10 points)   Write a linear program to determine a probability distribution over your $n$ choices that will maximizes your probability of a goal,  given that your opponent knows your strategy and plays optimally for that strategy.  Hint: let $q_{i}$ be your probability of kicking to region $i$.


\ans{
Maximize w\\
Subject to:\\
$$ \textrm{For all } 0 \leq i \leq n-1 0 \leq q_{i} \leq 1$$
$$ q_{n} = 1 - \sum_{i=1}^{n-1} q_{i} $$
$$ \textrm{For all } 0 \leq i \leq n-1,\  w \leq \sum_{j=1}^{n}  q_{j}p_{j,i} $$ 
}


\end{enumerate}

 
 
 \problem{Network Flow and Applications}
 
 \begin{enumerate}
 
A computer $s$ is initially infected with a virus in a directed network and there is a critical computer $t$ that you want to ensure will not become infected.  You are able to shut down any computers except for $s$ and $t$, and you want to prevent infection by shutting down the minimum number of computers such that there is no longer any path from $s$ to $t$.

 \item (10 points) Given a directed network $G$, and special nodes $s$ and $t$, describe an algorithm to find the minimum number of computers that must be shut down to prevent the spread of infection from $s$ to $t$.
 
 \ans{Create a new graph $G'$ where for each vertex $v$ in $G$, $G'$ has two vertices $v_{a}$ and $v_{b}$, with an edge of weight $1$ from $v_{a}$ to $v_{b}$.  All edges in $G$ that pointed to $v$ previously now point to $v_{a}$ and all edges from $v$ in $G$ now are from $v_{b}$.  Further, all these edges have a weight of infinity.  Now find the min cut in $G'$}
 
 
 \pagebreak
 
 
 \item (15 points) Assume you are given a directed graph $G$ where every vertex has out-degree $k$ and in-degree $k$ for some particular value $k$.  Describe an algorithm to output a collection of cycles such that every vertex in the graph is in exactly one cycle in the collection.  Show that your algorithm is correct.  Hint: Bipartite Matching.

\ans{Set up bipartite graph with all nodes in $G$ on left side and copies of all nodes on right.  Edges in this graph are edges from $G$, all going from left to right.  Because the graph is $k$ regular, Hall's Theorem applies.  Why?  We can thus find a perfect matching using network flow and this matching will induce the collection of cycles desired.}

\end{enumerate}


\problem{Randomized Algorithms}


Consider a situation where we have $n$ servers and $n$ clients.  The servers all know a message $m$ and the clients want to learn that message.  Our goal is to design an algorithm that ensures that all clients learn $m$, while sending the smallest number of messages possible.  We have access to a global random number generator $R$ that generates a number uniformly at random between $1$ and $\sqrt{n}$, and which all the servers can read.

Consider the following algorithm:

\begin{enumerate}
\item Each client chooses a subset $S$ of $\sqrt{n}$ of the $n$ servers, uniformly at random from all such subsets.  For each $s \in S$, the client then generates $\sqrt{n}$ requests of the form $(p,t)$, by choosing independently a random tag $t$ that is an integer distributed uniformly at random between $1$ and $\sqrt{n}$, and sends the request $(s,t)$ to server $s$
\item A random number $r$ is generated by the global random number generator and all servers read that number
\item Every server $s$ considers the requests they have received of the form $(s,r)$.  If there are less than $k \sqrt{n}$ such requests, then $s$ sends $m$ to each client that it received such a request from.  $k$ is a fixed constant.
\end{enumerate}

Unfortunately, some of the clients are \emph{bad} in that they may disregard the first line of the algorithm, sending out more than $\sqrt{n}$ requests, which may not necessarily be generated randomly.  Note that the number of messages sent by each good client and server is always only $O(\sqrt{n})$.  In this problem, you will show that even with the bad clients around, and even with this bound on communication costs, the protocol still has a good chance of ensuring all the good clients will learn $m$.

Call a sever \emph{overloaded} if in the last step of the algorithm, it receives more than $k\sqrt{n}$ requests.  Note that you can assume that each server receives at most $n$ requests, since if they receive two requests from the same client, they can assume that client is bad and just throw out its requests.  
\begin{enumerate}

\item (5 points) Derive an upper bound on the probability that a fixed server is overloaded.
\ans{There are at most $n$ requests total that each server receives and these are distributed over $\sqrt{n}$ possible tags.  Thus, at most $\sqrt{n}/k$ of the tags have more than $k \sqrt{n}$ requests.  Thus the probability of being overloaded is at most $1/k$} \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\
  
\pagebreak 
  
\item (5 points) Give an upperbound on the expected number of servers that are overloaded.  Note: The events that two different servers are overloaded are NOT independent.

\ans{Using linearity of expectation, the expected number of informed processors that are overloaded is at most $n/k$}  \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\

\item (5 points) Now use Markov's inequality to bound the probability that the number of overloaded servers is greater than or equal to $n/6$.  If everything is going well, you should be able to show this probability is no more than $6/k$.
\ans{Let $X$ be the number of overloaded servers.  By Markov's, $Pr(X \geq \lambda) \leq E(X)/\lambda$, which means $Pr(X \geq n/6) \leq (n/k)/(n/6) = 6/k$.} \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\


\pagebreak

\item (5 points) Now assuming that at most $n/6$ servers are overloaded, calculate the probability that a given client fails to send a request to any server that is not overloaded.  Note: The servers that a single client sends requests to are NOT chosen independently (since a given server can not be chosen more than once).  You may find the following bound from the book helpful: 
$$ (x/y)^{y} \leq {x \choose y} \leq (xe/y)^{y}$$

\ans{The probability of sending every request to a processor that is overloaded is ${n/2 \choose \sqrt{n}} / {n \choose \sqrt{n}}$.  Using the above inequality, this probability is no more than $(e/6)^{\sqrt{n}} \leq (1/2)^{\sqrt{n}}$} \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\

\pagebreak

\item (5 points) Finally, use union bounds and the above results to bound the probability that \emph{any} good client does not receive the message.  For $n$ large, what is a good approximation to the probability of failure?

\ans{The probability of failure is at most $6/k + n*(1/2)^{\sqrt{n}}$.  Since $n = 2^{log n}$, we have $6/k + 2^{\log n - \sqrt{n}}$.  For $n$ large, $6/k$ is a good approximation to the probability of failure.}



\end{enumerate}


 
 
 
 
\end{enumerate}
\end{document}




\problem{Short Answer}\\

For each problem below, give the answer in terms of simplest $\Theta$.  Please show your work where appropriate. (2 points each).

\begin{enumerate}

\item Worst case time to find the minimum element in a min-heap \ans{$\Theta(1)$}

\ \\ \ \\ \ \\ \ \\ \ \\

\item Worst case time to find the maximum element in a min-heap \ans{$\Theta(n)$ - The maximum element could be anywhere on the bottom level}
 
\ \\ \ \\ \ \\ \ \\ \ \\

\item Worst case cost of $n$ calls to either Insert or Delete on the most efficient implementation of the dynamic table that we discussed in class \ans{$\Theta(n)$}
 
\ \\ \ \\ \ \\ \ \\ \ \\


\item $\sum_{i=1}^{n} \log i$ \ans{$\Theta(n \log n)$}

\ \\ \ \\ \ \\ \ \\ \ \\

\item Time to find a minimum spanning tree in a graph with $n$ nodes and $\Theta(n^{2})$ edges, using the fastest algorithm discussed in class

\ans{Prim's algorithm takes time $O(|E| + |V| \log |V|)$ which is $O(n^{2})$ in this case}

\ \\ \ \\ \ \\ \ \\ \ \\

\item Time to find if a cycle is reachable from some node $v$ in a directed graph with $n$ nodes and $O(n)$ edges.

\ans{$\Theta(n)$ - this can be done with BFS starting at $v$ and checking if there are any back edges.} 

\ \\ \ \\ \ \\ \ \\ \ \\

\item Solution to the recurrence $T(n) = 2T(n/2) + \sqrt{n}$ \ans{$\Theta(n)$}

\ \\ \ \\ \ \\ \ \\

\item Expected number of empty bins if you randomly throw $2n$ balls into $n$ bins.
\ans{This is $n(1-1/n)^{2n} \leq ne^{-2}$ which is $\Theta(n)$}

\ \\ \ \\ \ \\ \ \\ \ \\

\item What is the expected time to find the successor of some key (i.e. the node that comes right after the key) in a skiplist containing $n$ items? \ans{$\Theta(\log n)$}
 
 \ \\ \ \\ \ \\ \ \\ \ \\
 
\item Solution to the recurrence $T(n) = 2T(n-1) - T(n-2) + n$ (assume all constants in the solution are greater than $0$) \ans{$\Theta(n^{3})$}

 
\end{enumerate}

\problem{Short Answer (10 points each)} {\bf Where appropriate, circle
your final answer.}
\begin{enumerate}

\item Professor Plum postulates that if every edge on an undirected graph has a unique positive weight, then the shortest path tree rooted at $v$ on that graph is always the same as the minimum spanning tree found by Prim's algorithm when seeded initially with the vertex $v$.  Is this correct?  If so, prove it.  If not, give a counter example.

\ans{No.  Consider the graph over vertices $v,w,x$ where edge $(v,w)$ has weight $1$, edge $(v,x)$ has weight $2.5$ and edge $(w,x)$ has weight $2$.  The shortest path tree for $v$ is edges $(v,w)$ and $(v,x)$.  The MST found by Prim's when started at $v$ is $(v,x)$ and $(w,x)$.}

\pagebreak

\item Consider a data structure over an initially empty list that supports the following two operations.  APPEND-NUMBER(x): Adds the number $x$ to the beginning of the list; and MIN-MAX: Traverses the list, computing the minimum and maximum element in the list, and then creates a new list that contains only two numbers, the minimum and maximum of the old list. 

\begin{itemize}
\item Assume an arbitrary sequence of $n$ operations are performed on this data structure.  What is the worst case run time of any particular operation? \ans{$\Theta(n)$.  First $n-1$ APPEND-NUMBER operations and then a MIN-MAX operation.}
\ \\ \ \\
\item Show that the amortized cost of an operation is $O(1)$ using the potential method.  Make sure to prove your potential function is valid.
\ans{Let $\phi(D)$ equal the number of items in the list.  This is a valid potential function (Why?).  The amortized cost of an APPEND-NUMBER operation is then $c_{i} + \phi_{i} - \phi_{i-1} = 2$.  The amortized cost of a MIN-MAX operation is $l_{i}+(2-l_{i}) = 2$ where $l_{i}$ is the length of the list at time $i$.}
\end{itemize}

\end{enumerate}


\problem{Segmentation}

In the segmentation problem, you want to segment text that is written without spaces into individual words.  For example, if you are given the text "meetateight", the best segmentation is "meet at eight", not "me et at eight" or "meet ate ight".  Assume you are given as a black box a function $q$ that takes a string of letters $x$ and returns $q(x)$, which gives a measure of how likely $x$ is to be an English word.  The quality of $x$ can be positive or negative so that $q("me")$ is positive, and $q("ight")$ is negative.  Given a string $s$, a segmentation of $s$ is a partition of the letters into contiguous blocks.  The total quality of a segmentation is the sum of the quality of each of the blocks of letters.  So the quality of our segmentation above is $q("meet") + q("at") + q("eight")$.  Give an efficient algorithm that takes as input a string $s$ of length $n$ and returns a segmentation of maximum total quality (assume that a single call to the $q$ function takes constant time).  Analyze your algorithm.

\ans{We will show only how to find the quality value of the best segmentation.  Finding the segmentation itself is straightforward once you know how to compute the optimal quality (you simple need to use another array to keep track of which arguments are used to achieve the maximum values in the following).  We will define $m(i)$ to be the maximum quality of a segmentation of $s[1..i]$.  Note that $m(1) = q(s[1])$ and that \\ $m(j) = \max_{j'<j} m(j') + q(s[j'+1..j])$.  We can easily create a dynamic program based on this recurrence by create an array $m$ of size $n$, and filling it in from left to right using the above recurrence.  The value returned is $m(n)$.  This dynamic program will have two loops (an outer loop to range from $j$ values from $2$ to $n$ an inner loop to range over $j'$ values from $1$ to $j' - 1$.) and so will have running time $O(n^{2})$. }




\problem{Goods Trading}

Assume there are $n$ goods $g_1,g_2,...,g_n$ and there is an $n$ by $n$ table $T$ such that one unit of good $g_i$ buys $T[i,j]$ units of good $g_j$.  Part 1: Give an efficient algorithm to determine whether or not there is a sequence of
goods $g_{i1},g_{i2},...,g_{ik}$ such that $T[g_{i1},g_{i2}]*T[g_{i2},g_{i3}]*...*T[g_{ik},g_{i1}] > 1$.  In other words, give an algorithm to determine if there is a sequence of goods that can be traded to actually obtain more of some good.  Analyze your algorithm.

\ans{Note that $T[g_{i1},g_{i2}]*T[g_{i2},g_{i3}]*...*T[g_{ik},g_{i1}] > 1$ is true iff $1/T[g_{i1},g_{i2}]*1/T[g_{i2},g_{i3}]*...*1/T[g_{ik},g_{i1}] < 1$.  Taking logs of both sides, we can express this inequality as $- log T[g_{i1},g_{i2}] - log
T[g_{i2},g_{i3}] - ... - log T[g_{ik},g_{i1}] < 0$.  Thus we can create a graph over $n$ vertices, one for each good.  And for every pair of goods $g_i$ and $g_j$, we can create an edge with weight $-log T[g_i,g_j]$.  There is a negative cycle in this graph iff there the desired sequence of goods exists.  We can use any of our standard algorithms for finding negative cycles since the graph is strongly connected.  If we use Bellman-Ford, our algorithm takes $O(n^3)$ time.}

\pagebreak

Part 2: Give an efficient algorithm to print out such a sequence of goods if one exists.  Analyze your algorithm. 

\ans{In the last stage of Bellman-Ford, if there is some edge that is tense,we can just select one such node, $(v_1,v_2)$ to be the first edge in the cycle.  We run Bellman-Ford another step to find the next edge, $(v_2,v_3)$ that points out
of $v_2$ that is tense.  We continue this process until we find all the edges in a cycle.  This will require running Bellman-Ford for at most $n$ additional steps so it still has a run time of $O(n^3)$.}


\problem{Bad Santa}

A Bad Santa has hidden $n/2$ Nintendo Wii's in $n$ boxes.  A child is presented with each box in sequence and must decide to either open that box immediately or to pass on the box and never be able to open it again.  The child wants to guarantee she get at least one Wii while opening the smallest number of boxes in expectation.  The Bad Santa knows the child's algorithm and places the Wii's in boxes so as to try to maximize the expected number of boxes opened.  In this problem, you must design and analyze an algorithm for the child that 1) guarantees that the child finds a Wii; and 2) minimizes the expected boxes opened until that Wii is found.  

Hint 1: Birthday paradox;  Hint 2: You may again find the following inequality
useful $1-x \leq e^{-x}$

A final note, not necessary for solving the problem: this problem has applications in designing power-efficient sensor networks (the boxes are time steps, opening a box means being awake for a time step, and the presents represent time steps when important data is broadcast)

\ans{Surprisingly, you can do much better than opening $\Theta(n)$ boxes in expectation - You can open $O(\sqrt{n \ln n})$ boxes in expectation.  The trick to this problem is the birthday paradox.  Recall that when we talked about the b-day paradox, we noticed that if you throw sqrt(n) balls into n bins, that you expect at least two balls to fall into one bin.  In this problem, such an event is equivalent to the girl opening a box that has a hidden present in it in the first half, provided that there are at least sqrt(n) presents in the first half.  In particular, the $\sqrt(n)$ boxes with prizes in them correspond to special bins.  We're throwing $\sqrt(n)$ balls into the $n/2$ total bins in the first half.  The probability that a fixed ball falls in a fixed special bin is 1/n.  The total number of combinations of balls and special bins is $\sqrt(n)*\sqrt(n) = n$, and so by linearity of expectation, the expected number of special bins with a ball in them is $1$.  So if there are $\sqrt(n)$ presents in the first half, and we open
$\sqrt(n)$ boxes randomly, we would expect to find at least one prize.\\
The trick is to then realize that if we open up slightly more than $\sqrt(n)$ boxes, i.e. if we open $\sqrt(n) \log n$ boxes then we can turn this expected event into one that will almost certainly occur.  Note that anything that is a bit bigger asymptotically than sqrt(n) works fine (e.g. $n^{1/2+\epsilon}$ for any $\epsilon>0$), and is good for partial credit, it's just that $\sqrt(n) \log n$ is nearly the smallest you can go.\\
The entire solution is then as follows.  Choose $\sqrt{n\ln n}$ boxes from the first $n/2$ boxes uniformly at random with replacement, then choose all of the last $n/2$ boxes.  Open each chosen box in turn until a present is found.  This algorithm opens no more than $\sqrt{n\ln n}$ boxes in expectation as we will now show by a case based
analysis.  Case 1: At least $\sqrt{n}$ of the first $n/2$ boxes are full.  In this case, the probability that the algorithm does not find a present in the first $n/2$ boxes is no more than $(1-1/\sqrt{n})^{\sqrt{n \ln n}} \leq e^{-\sqrt{\ln n}} \leq 1/\sqrt{n}$.  Thus the expected number of boxes the algorithm opens is no more than $\sqrt{n \ln n} + (1/\sqrt{n})*(n/2) = O(\sqrt{n \ln n})$.  Case 2: Less than $\sqrt{n}$ of the first $n/2$ boxes are full.  In this case, the algorithm opens at most $\sqrt{n\ln n}$ boxes in the first half and at most $\sqrt{n}$ boxes in the second half (since at most $\sqrt{n}$ boxes in the second half can be empty.  Thus in this case also the expected number of boxes opened is $O(\sqrt{n \ln n})$}
 
 
   
\clearpage \ \\

\end{enumerate}

\end{document}

Solve the following recurrence using annihilators: $T(n) =
 4T(n-2) + n$.  Give the solution in general form i.e. do not
solve for the constants\\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\
\ans{The annihilator of the homogeneous part is $\Le^{2} -4$ which factors to $(\Le
-2)(\Le +2)$.  The annihilator of the non-homogeneous part is $(\Le -1)^{2}$. 
This implies that the general solution is $T(n) = c_{1}2^{n}+c_{2}(-2)^{n} + c_{3}n +c_{4}$}

\problem{Quicksort}
 
You are working on the problem of developing a new randomized PARTITION \footnote{Not necessarily comparison based} algorithm for quicksort (Section 7.1 in the text).  In this problem, you will consider how improving the effectiveness of PARTITION might change the runtime of quicksort.  Recall that the PARTITION function splits an array into two subarrays, $A_{1}$ and $A_{2}$, such that any element in $A_{1}$ is less than or equal to any element in $A_{2}$.

 
 \begin{itemize}
 
 \item (5 points) Imagine that you create a randomized partition function with the following properties: 1) its expected run time is $\theta(\sqrt{n})$; and 2) it always partitions into two subarrays that differ in size by at most $1$ (i.e. each subarray is of size no more than $n/2$).  If you use this new partition function in quicksort, what is the new expected run time?
 
 \ans{Expected runtime is given by the recurrence $T(n) = 2T(n/2) + \sqrt{n}$.  By the Master's method, the solution to this is $T(n) = \theta(n)$.}
 
 \item (20 points) Now imagine that your randomized partition function has the following properties: 1) its expected run time is $\theta(1)$; and 2) for $i$ selected uniformly at random between $1$ and $\sqrt{n}$, one subarray is of size no more than $n/2 - i$ and the other subarray is of size no more than $n/2 + i$.  Show by induction that the expected runtime of quicksort is now linear.  Hints: (1) This is a variation of Problem 7-2; (2) In your inductive proof, you may find it easiest to show that the expected run time is no more than $k_{1}n - k_{2})$ for constants $k_{1}$ and $k_{2}$ (see the ``Subtleties'' subsection in Chapter 4.1 for details).
  
 \ans{This is a variation on Problem 7-2.  Let $f(n)$ be the expected run time of quicksort now on a list of size $n$.  Then via linearity of expectation, we have
 \begin{eqnarray*}
 f(n) & \leq & \sum_{q=- \sqrt{n}} ^{\sqrt{n}} (1/2\sqrt{n})( f(n/2 + q) + f(n/2 - q) + c) \\
  f(n) & \leq & (1/\sqrt{n}) \sum_{q= -\sqrt{n}} ^{\sqrt{n}}  (f(n/2 + q) + c) \\
\end{eqnarray*}
Let's guess that $f(n) \leq k_{1} n - k_{2}$ for some constants $k_{1}$,$k_{2}$ and then prove this by induction.\\
Base Case: $f(1) = \theta(1) \leq k_{1}n - k_{2}$ for $k_{1}$ sufficiently large compared to $k_{2}$\\
I.H.: For all $j <n$, $f(j) \leq kj$\\
I.S.: 
\begin{eqnarray*}
  f(n) & \leq & (1/\sqrt{n}) \sum_{q= -\sqrt{n}} ^{\sqrt{n}}  (f(n/2 + q) + c) \\
  & \leq & (1/\sqrt{n}) \sum_{q= -\sqrt{n}} ^{\sqrt{n}}  (k_{1}(n/2 + q) - k_{2} + c)\\
  & \leq & (1/\sqrt{n}) \sum_{q= -\sqrt{n}} ^{\sqrt{n}}  (k_{1}(n/2 + q)\\
  & \leq & (2\sqrt{n} + 1)(1 + k\sqrt{n}/2) + (k/\sqrt{n}) \sum_{q= 0} ^{\sqrt{n}}  q\\
  & \leq & kn
\end{eqnarray*}
 Where the second step follows by the inductive hypothesis; the third step follows for $k_{2}$ sufficiently large; and the last step holds for $k$ sufficiently large.
 }
 
 \end{itemize}
 
  to have created a (non-comparison based) algorithm that can 
 
 do the following when given a list.  It returns an element selected uniformly at random from the set of all elements in the list that 
 

