%\newcommand{\ans}[1]{}
\documentclass[11pt]{article}
\usepackage{fullpage,pst-all,epsfig}

\usepackage{algorithmic}
\usepackage{algorithm}

\usepackage[margin=1in]{geometry}% http://ctan.org/pkg/geometry
\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\usepackage{graphicx}

\usepackage{caption}
\usepackage{subcaption}

\newcommand{\comment}[1]{}
\newcommand{\Le}{\textbf{L}}

%\newcommand{\ans}[1]{\emph{Solution: #1}}
\newcommand{\ans}[1]{}

\newcommand{\seq}[1]{ \langle #1,\cdots \, \rangle}
\newcommand{\seqi}[1]{ \langle #1 \rangle}
	
\begin{document}
\thispagestyle{empty}
\begin{center}
\def\handout{Midterm Examination}
\vspace*{-.75in}
{\large University of New Mexico}\\
{\large Department of Computer Science}\\
\vspace*{0.5in}
{\LARGE {\bf \handout}}\\
\vspace*{0.1in}
{\large CS 561 Data Structures and Algorithms}\\
{\large Fall, 2012}\\ [0.3in]
\end{center}
 
\vfill

\makeatletter
\long\def\hint#1{({\em Hint\/}: #1)}
% \def\@oddhead{\rm\makebox[0in][l]{CS 461 Midterm ---Fall,
% 2003}\hfil\thepage\hfil\makebox[0in][r]{Name:\rule[-0.1in]{2in}{.5pt}}}
\let\@evenhead\@oddhead
\def\@oddfoot{}
\let\@evenfoot\@oddfoot

\def\problem#1{\def\problemheading{#1}\clearpage\item{\bf #1}}

% Comment out the above 'problem' def and use the one below to get
% all the problems on a single page, instead of page break each time.
%\def\problem#1{\def\problemheading{#1}\item{\bf #1}}

\def\extrapage{\addtocounter{enumi}{-1}\clearpage\item{\bf \problemheading, continued.}}

\let\part\item
\renewcommand{\theenumii}{\alph{enumii}}
\makeatother
\parindent 0pt

\vfill
\centerline{
\Large
\begin{tabular}{|l|}  \hline
Name: \hspace*{2in} \\ \hline
Email: \hspace*{2in}\\ \hline
\end{tabular}
}
\vfill

\hrule
\begin{itemize}

\item This exam lasts 75 minutes.  It is open book and open notes but no electronic devices are permitted.

\item {\em Show your work!}  You will not get full credit if we cannot figure out how you arrived at your answer.  

\item Write your solution in the space provided for the corresponding problem.

\item If any question is unclear, ask for clarification.

\end{itemize}
\hrule
\vfill
\centerline{
\Large
\begin{tabular}{|c|c|c|c|}  \hline
Question & Points & Score & Grader \\  \hline\hline
1 & 20 & & \\  \hline
2 & 20 & & \\  \hline
3 & 20 & & \\  \hline
4 & 20 & & \\  \hline
5 & 20 & & \\  \hline
\hline Total & 100 & & \\  \hline
\end{tabular}
}
\vfill

\newpage

\begin{enumerate}
 

\problem{Short Answer}
 
 Answer the following questions using simplest possible $\theta$ notation.  Draw a box around your final answer.  No need to justify answers for problems on this page.
 
 \begin{enumerate}

\item Solution to the following recurrence relation: $f(n) = 3f(n/2) + n$. \ans{$\theta(n^{\log_{2} 3})$} \\ \ \\ \ \\ \ \\ \ \\

\item Solution to the following recurrence relation: $f(n) = 5f(n-1) - 6f(n-2) + n$. \ans{$\theta(3^{n})$ or $\theta(c_{1}2^{n} + c_{2}3^{n} + c_{3}n + c_{4})$.  Annihilator is $(L - 2)(L-3)(L-1)^{2}$.} \\ \ \\ \ \\ \ \\ \ \\

%\item Solution to the following recurrence relation: $f(n) = 2f(\sqrt{n}) + n$. \ans{$\theta(n)$.  Substitute $n = 2^{i}$.  Then $f(2^{i}) = 2 f(2^{i/2}) + 2^{i}$ so $F(i) = 2 F(i/2) + 2^{i}$ and $F(i) = 2^{i}$.  The reverse substitution gives that $F(i) = \theta(n)$.} \\ \ \\ \ \\ \ \\ 


\item Number of balls that must be thrown uniformly and independently into $n$ bins before we expect at least one pair of balls to fall in the same bin. \ans{$\theta(\sqrt{n})$.  This is what we showed in the birthday paradox analysis.} \\ \ \\ \ \\ \ \\

\item Runtime of fastest algorithm to solve the Fractional Knapsack problem over $n$ when the value per pound of each item is distributed independently and uniformly at random between $0$ and $1$. \ans{$\Theta(n)$ since can use bucket sort to do the sorting.} 

\pagebreak

\item Consider the recurrence $f(1) = 2$, $f(n) = f(n/2) * f(3n/4)$.  Prove via induction that $f(n) \geq 2^{n}$.  Don't forget the base case (BC), inductive hypothesis (IH) and inductive step (IS).  Also make it clear where you are using the IH in the IS.

\ans{BC: $f(1) = 2 \geq 2^{1}$.  IH:  $\forall_{j < n}, f(j) \geq 2^{n}$.  IS: $f(n) = f(n/2)*f(3n/4) \geq 2^{n/2}*2^{3n/4} \geq 2^{n}$.  The second step holds via the IH.}
  



 \end{enumerate}


\problem{Amortized Analysis}

Recall that in class we discussed an INCREMENT algorithm for incrementing a binary counter in $O(1)$ amortized time.  Now suppose we want to also support a RESET operation that sets all the bits in the counter to $0$.  Below are the algorithms for INCREMENT and RESET.  They make use of an array B of bits and a variable m which is the most significant bit.


%\begin{figure}[ht]

\begin{tabular}{cc}
 \begin{minipage}[t]{3.0in}
\centering
\begin{algorithm}[H]
\caption{INCREMENT($B[0,\ldots,\infty],m$)}
\label{a:phase}
%\begin{boxedminipage}{\textwidth}

%\textbf{\\Assumptions:}
 \begin{algorithmic}[1]
%\COMMENT Processor $p$: initial value $i_p$
\STATE $i \leftarrow 0$
\WHILE{$B[i] = 1$} 
\STATE $B[i] \leftarrow 0$
\STATE $i \leftarrow i + 1$
\ENDWHILE
\STATE $B[i] \leftarrow 1$
\IF{$i > m$}
\STATE $m \leftarrow i$
\ENDIF 
 \end{algorithmic}
%\end{boxedminipage}
\end{algorithm}

 \end{minipage}

&

 \begin{minipage}[t]{3.00in}
\centering
\begin{algorithm}[H]
\caption{RESET($B[0,\ldots,\infty],m$)}
\label{a:phase}
%\begin{boxedminipage}{\textwidth}

%\textbf{\\Assumptions:}
 \begin{algorithmic}[1]
%\COMMENT Processor $p$: initial value $i_p$
\FOR{$i \leftarrow 0$ to $m$}
\STATE $B[i] \leftarrow 0$
\ENDFOR

 \end{algorithmic}
%\end{boxedminipage}
\end{algorithm}

\end{minipage}
\end{tabular}
%\end{figure}

\medskip

In the following questions, let $n$ be the number of operations on this binary counter.

\begin{enumerate}

\item What is the worst-case run time of INCREMENT as a function of $n$? \ans{$\theta(\log n)$}\\ \ \\ 

\item What is the worst-case run time of RESET as a function of $n$? \ans{$\theta(\log n)$}\\ \ \\ 

\item Prove that in an arbitrary sequence of calls to INCREMENT and RESET, each operation has an amortized cost of O(1). 
\ans{Accounting method.  We maintain the invariant that: a) each 1 bit has a dollar on it; and b) that we always have a separate ``reset pool'' containing at least \$m's to pay for a RESET.  We charge INCREMENT \$3 and RESET \$1.  On a call to  INCREMENT (as in class), we spend \$1 in flipping a single 0 to a 1 and use the dollars stored on the 1 bits to flip them to 0's.  Finally, we add the remaining \$1 to the ``reset pool''.  On a call to RESET, we spend the \$1 charged immediately to pay for the call overhead.  We then use the money stored in the RESET pool to pay for the cost of setting $m$ bits to $0$.  Note that the number of dollars in the ``reset pool'' is at least $2^{m} \geq m$.  Thus there is always enough in this pool to pay for the cost $m$ of setting $m$ bits to zero.}

\end{enumerate}

\extrapage


\problem{Probability}

Imagine we place $m$ sensor nodes independently and uniformly at random in the cells of a $n$ by $n$ array.  We say that a pair of sensors \emph{conflict} if they are in the same row or in the same column.

\begin{enumerate}

\item What is the expected number of pairs of sensors that conflict? \ans{Let $X$ be the number of pairs of sensors that conflict.  For some pair of castles $i,j$, let $X_{i,j}$ be an indicator variable that is $1$ if the pair can attack and $0$ otherwise.  Then $E(X) = \sum_{i,j} E(X_{i,j})$ by linearity of expectation.  Note that $E(X_{i,j}) = (2n-1)/n^{2}$.  Thus, $E(X) = (m(m-1)/2) (2n-1)/n^{2}$.} \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\





\item How large must $m$ be for the expected number of conflicting pairs to be greater than $1$. Give your answer in $\theta()$ notation as a function of n. \ans{$\theta(\sqrt{n})$}


\end{enumerate}





\problem{Donuts Revisted}

Assume there are three types of donut boxes: a box containing $x_{1} = 1$ donut costing $c_{1}$ dollars; a box containing $x_{2}$ donuts costing $c_{2}$ dollars; and a box containing $x_{3}$ donuts costing $c_{3}$ dollars.  You want to buy exactly $n$ donuts at minimum cost.  (Note that since $x_{1} = 1$, you can now always buy exactly $n$ donuts for any positive $n$.)

\begin{enumerate}
\item Let $m(i)$ be the minimum cost of exactly $i$ donuts.  Write a recurrence relation for $m(i)$.  Don't forget the base case(s). 
\ans{$m(0) = 0$.  For all $i<0$, $m(i) = \infty$.  For all $i>1$, $m(i) = min (m(i-1)+c_{1}, m(i-x_{2})+c_{2}, m(i-x_{3}) + c_{3})$} \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\



\item Now describe in 1-3 sentences how you would create an algorithm to compute the minimum cost way to buy exactly $n$ donuts.  We only want a high level description, not the entire algorithm.  You will loose points on this answer if you write more than $3$ sentences.
\ans{Create an array $m$ of size $n$, fill it in from left to right using the above recurrence, and return the value $m[n]$.} \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\


%\item What is the runtime of your algorithm?
%\ans{$\theta(n)$} \\ \ \\ 

\pagebreak

\item Now describe in 3-4 sentences how you would amend your algorithm so that it also returns the set of boxes that achieves the minimum cost.  You will loose points if you write more than 4 sentences.

\ans{Create a new array $s$ of size $n$.  When filling in the values for $m(i)$, set $s(i)$ to be the number of donuts in the box that achieves the minimum.  After $m[n]$ is computed, set $j \leftarrow n$.  Then until $j=0$, output the value in $s[j]$, and set $j \leftarrow j - s[j]$.}


\end{enumerate}


%\extrapage


 

\problem{Drunken Debutants}

Recall that in the homework, we had $n$ debutants (debs) randomly assigning themselves to $n$ porsches.  Here we consider a new variant.  Assume that both the debs and porsches are labelled $1$ through $n$, where porsche $i$ belongs to debutante $i$.  Now the debs leave the party in order from $1$ to $n$.  Deb 1 is drunk and enters a porsche uniformly at random.  Then for $i = 2, 3, \ldots n$, deb $i$ first checks porsche $i$.  If it's empty, she stays there, if it's full, she enters a porsche chosen uniformly at random from all empty porsches.

In this problem, you will calculate the value $f(n)$, the probability that the last deb enters her own porsche.

\begin{enumerate}

\item Note that $f(1) = 1$.  What is $f(2)$? \ans{$f(2) = 1/2$} \\ \ \\ \ \\ \ \\ \ \\ \ \\

\item Write a recurrence relation for $f(n)$.  Hint 1:  Deb 1 enters porsche 1 with probability $1/n$.  What is $f(n)$ in this case?  How about if deb $1$ enters porsche $2$?  In that case, can you write $f(n)$ as $f(j)$ for some $j < n$.  Hint 2: The right hand side of
your recurrence should contain many $f(j)$ values.  \\ \ \\ \ \\ 

\ans{The key here is to note that Deb 1 enters porsche $i$ with probability $1/n$.  If she enters porsche 1, $f(n) = 1$.  If she enters porsche $n$, $f(n) = 0$.  If she enters porsche $1 < i < n$ then $f(n) = f(n-(i-1))$.  To see this note that Debs 1 through (i-1) will all go to their own porsches, and the Deb i will choose a random porsche among the $n-(i-1)$ that remain.  The recurrence is thus:
$f(n) = (1/n) (1 + \sum_{i=1}^{n-2} f(n-i)) = (1/n) (1 + \sum_{i=2}^{n-1} f(i))$}



%Deb 1 enters porsche $i$ with probability $1/n$ for each $i$ from $1$ to $n$. 

\pagebreak

\item Now use the guess and check method to solve for the exact value of $f(n)$ for $n \geq 2$.  Don't forget to include base case, IH, and IS.\\ Hint: To get a good guess, you may want to compute $f(3)$ and $f(4)$ from the recurrence in order to spot a trend.

\ans{Guess: $f(n) = 1/2$ for $n \geq 2$.  BC: $f(2) = 1/2$.  IH: $\forall_{2 \leq j <n} f(j) = 1/2.$\\  IS:  $f(n) = (1/n) (1 + \sum_{i=2}^{n-1} f(i)) = (1/n) (1 + (n-2)/2) = 1/2$.  Here the second to last step holds via the inductive hypothesis.}


\end{enumerate}

\end{enumerate}

  
  
 
  \end{document}
 
 
 
 
In this problem, you have a $n$ wireless sensors located in a network.  When two adjacent nodes are assigned the same channel, there is a certain amount of interference, which is given by the weight on the edge between the adjacent nodes.  An assignment for the network is an assignment of a channel to each node in the network, and the total cost of an assignment is the sum of the interference costs for all adjacent nodes.  Your goal in the problems below is to find an assignment that minimizes total cost.\\
  
 {\bf For each of the variants below, 1) give a recurrence relation for the desired value; 2) describe a dynamic program; and 3) give an analysis of the runtime of your dynamic program.}
  

  \begin{enumerate}
  \item (6 points) Imagine that the $n$ sensors are connected in a line, and that for all $1 \leq i < n$ the edge between node $i$ and node $i+1$ has positive weight
  $w(i-1,i)$.  Hint: Let $c(i,1)$ be the minimum cost for assigning channels to nodes $1$ through $i$ when node $i$ is assigned channel $1$.  Let $c(i,2)$ be the minimum cost of assigning channels to nodes $1$ through $i$ when node $i$ is assigned channel $2$.
  
  \ans{The recurrence relation is the following:\\
$c(1,1) = c(1,2) = 0$\\
$c(i,1) = \textrm{min } (c(i-1, 1) + w(i-1,i), c(i-1, 2))$\\
$c(i,2) = \textrm{min } (c(i-1, 1), c(i-1, 2) + w(i-1,i))$
The dynamic program just keeps an array of size $n$ with all these values and fills it in from left to right.  The final value returned is the minimum of $c(n,1)$ and $c(n,2)$.  Runtime is $O(n)$.
}

\pagebreak

\item (7 points) Now assume that the sensors are connected in a binary tree.  Hint: for node $v$ in the tree, let $c(v,1)$ be the min cost for assigning channels to all nodes in the subtree rooted at $v$ when $v$ is assigned channel $1$, and define $c(v,2)$ similarly.  Let $\textrm{left(v)}$ ($\textrm{right(v)}$) be the left (reps. right) child of $v$ if they exist or NIL otherwise; let $w(x,y) = 0$ if either $x$ or $y$ is NIL.
  
\ans{Base Case is when $v$ is a leaf node or when $v$ is NIL.  Then $c(v,1) = c(v,2) = 0$.  When $v$ is not a leaf node, we have the following recurrence:\\
$c(v,1) = \textrm{min } (c(left(v),2) + w(v,left(v))), c(left(v),1)) + \textrm{min } (c(right(v),2) + w(v,right(v))), c(right(v),1))$
$c(v,2) = \textrm{min } (c(left(v),1) + w(v,left(v))), c(left(v),2)) + \textrm{min } (c(right(v),1) + w(v,right(v))), c(right(v),2))$
Dynamic program just stores these values for each node in the tree, working from the leaf nodes up.  Runtime is $O(n)$.
}  

\pagebreak
  
\item (7 points) Finally, assume that the sensors are connected in a binary tree and that at least a $2/3$ fraction must be assigned channel $1$.  Hint: Add another parameter to the function $c$.  For this problem, you only need to write down the recurrence relation for $c$ and give a very brief sketch of the algorithm and its runtime.
\ans{Now we let $c(v,x,c)$ be the min cost for assigning channels to all nodes in subtree rooted at $v$ if 1) $v$ is assigned channel $c$; and 2) exactly $x$ nodes in the subtree are assigned channel $c$.  
We will keep a $3$ dimensional table that is of size $n$ by $n+1$ by $2$.  Initially, all entries of the table are set to infinity.  Base Case is when $v$ is a leaf node: $c(v,1,1) = 0$,  $c(v,0,1) = 0$.  When $v$ is not a leaf, we have the following recurrence, for all values $0 \leq k \leq n$:\\
$c(v,k,1) = \textrm{min }_{i,j s.t. i+j=k-1} ((\textrm{min } (c(left(v), i, 2) + w(v,left(v))), c(left(v), i,1)) + \textrm{min } (c(right(v), j, 2) + w(v,right(v))), c(right(v), j, 1))$
$c(v,i,2) = \textrm{min }_{i,j s.t. i+j=k} \textrm{min } (c(left(v), i, 1) + w(v,left(v))), c(left(v), i, 2)) + \textrm{min } (c(right(v), j, 1) + w(v,right(v))), c(right(v), j, 2))$
The dynamic program just stores these values for each node in the tree, working from the leaf nodes up.  Runtime is now $O(n^{3})$ since there are now $O(n^{2}) entries and each takes at most $O(n)$ time to fill in.  The minimum cost returned is the minimum over all $k \leq (2/3)n$ and $1 \leq x \leq 2$ of $c(v,k,x)$, where $v$ is the root of the tree.}

 
  \problem{Recurrences and Asymptotics}
 
 Remember that when the base case for a recurrence is not explicitly given, assume that it is constant for inputs of constant size.
 
 \begin{itemize}
 
 \item Consider the recurrence $f(0) = 1$, $f(n) = \sum_{i=0}^{n-1} f(i)$.  Prove that the solution to this recurrence is $\theta(2^{n}$)
 
 \ans{This is a straightforward proof by induction.}
 \ \\ \ \\  \ \\ \ \\ \ \\  \ \\ \ \\ \ \\  \ \\ \ \\ \ \\  \ \\ \ \\ \ \\  \ \\ \ \\ \ \\  \ \\

\item Solve the following recurrence: $f(n) = 4f(n-1) - 4f(n-2) + 2^{n}$.  Do not solve for the constant coefficients
\ans{$(L^{2}-4L+4) = (L-2)^{2}$ annihilates the homogeneous part and $(L-2)$ annihilates the non-homogeneous part.  So the annihilator is $(L-2)^{3}$ and hence the solution is $(c_{1}n^{2} + c_{2} n + c_{3}) 2^{n}$}
 \ \\ \ \\  \ \\ \ \\ \ \\  \ \\ \ \\ \ \\  \ \\ \ \\ \ \\  \ \\

\pagebreak


\item Solve the following recurrence: $f(n) = f(n/2) + f(n/4)$.  Do not solve for the constant coefficients.  If an algorithms runtime is given by this recurrence, how would it compare with algorithms with runtimes of $\theta(2^{n})$, $\theta(n)$, $\theta(\sqrt{n})$?

 \ans{Let $2^{i}=n$ and $F(i) = f(2^{i})$.  Then we get a transformed recurrence: $F(i) = F(i-1) + F(i-2)$, whose solution is  $F(i) = c_{1} \phi^{i} + c_{2} \hat{\phi}^{i}$. Reverse transforming this solution, and using the fact that $i = \lg n$ we get  
 $f(n) = n^{\lg \phi} + n^{\lg \hat{\phi}}$.  This solution is approximately $\theta(n^{.694})$, so it is better than linear but worse than square root run time.}
 \ \\ \ \\  \ \\ \ \\ \ \\  \ \\ \ \\ \ \\  \ \\ \ \\ \ \\  \ \\
 
 \end{itemize}

\problem{Heaps and Sorting}

Professor Humbert claims to have invented a new Max-Heapify function for heaps that runs in $O(\lg \lg n)$ time.  Moreover, Humbert claims that his new Max-Heapify function is completely comparison based, i.e. the only way it ever compares two keys is with the $\leq$ operator.  Could Humbert be right?

\ans{No.  If such a Max-Heapify function existed, we could plug it into heap sort and use if to do comparison-based sorting in $\Theta(n\lg \lg n)$ time. }

 \ \\ \ \\ \ \\  \ \\ \ \\ \ \\  \ \\ \ \\ \ \\  \ \\

Prove that the following algorithm correctly sorts a list of $n$ elements by induction on $n$.  Don't forget to include the base case, inductive hypothesis and inductive step.
\begin{verbatim}
SillySort(A, i, j){
  Let n = j - i + 1;                   // n is the number of elements to sort
  If n <= 1 then return;          //nothing to sort
  If n == 2                              // only two elements to sort
    then
      if A[i] > A[j], swap A[i] and A[j]
  else                                     // more than two elements to sort                    
     SillySort(A,i+n/2,j);         //SillySort the last n/2 elements of A
     SillySort(A,i,i+n/2);    //SillySort the first n/2+1 elements of A
     SillySort(A,i+1,j)             //SillySort the last n-1 elements of A
 }
\end{verbatim}

\extrapage

\ans{Prove by induction on the size of L.  Need to first prove that the minimum element in the array winds up in the first position.}

\problem{Data Structures}

\begin{itemize}

\item Imagine that in a red black tree, the rule that a red node must have two black children is relaxed to the rule that a red node must have at least one black child.  All other rules remain the same.  What is now the maximum asymptotic height that a red black tree with $n$ nodes can have?  Give an example or proof as necessary to justify your claim.

\ans{The tree can now have $\theta(n)$ height.  Imagine one branch where all the nodes are red except for the root, and one child of each node on the branch - the final node in this branch has two black children.}

\pagebreak

\item Imagine you have a ``blacklist'' of $n$ messages that are spam.  When a new message arrives in your mail queue, you want to test it to see if it's a message in your black list.  However, space is at a premium and so you don't want to have to store all $n$ messages in memory (assume each message is many bits in length).  Answer the following questions with asymptotic notation.  Hint: Assume that you have a hash function that maps strings of any size to integers that are essentially random.
\begin{itemize}
\item What is the minimum number of bits of space to ensure that you can test if new messages are in the blacklist and get false positive with probability no more than $1/100$?  What is the time needed to test if a message is in the blacklist in this case?
\item How many bits do you need if you want the probability of false positives to be no more than $1/n$?  What is the time to test membership in this case?
\end{itemize}
\ans{Use a Bloom filter.  You need $\theta(n)$ bits and $\theta(1)$ time in the first case (since $k = \theta(1)$).  In the second case, you need 
$\theta(n \lg n)$ bits (since $m$ must be $n \lg n$) to get probability of false positives less than $1/n$; you need $O(\lg n)$ time since $k$ is $O(\lg n)$}

\end{itemize}

 \problem{Probability}
 
 \begin{itemize}
 \item \emph{Bad Santa II: Mr. Kringle's Revenge:} Consider the Bad Santa problem from hw1 with the following change.  The child is allowed to take $2$ passes over the sequence of boxes if necessary, but still needs to find just one present before stopping.  However, Santa can secretly hide the presents again after the first pass of the child.  Describe and analyze an algorithm for this new problem that minimizes the expected number of boxes opened.  Hint: The problem is now much easier than the hw problem.
 
 \ans{The algorithm: In this first pass, choose $\log n$ boxes uniformly at random and open those.  In the second pass, just open all boxes from let to right until finding a present.  The expected number of boxes opened is no more than $\log n + (1/n)n$, since the probability of getting to the second pass is only $(1/2)^{\log n} = 1/n$.  Thus the expected number of boxes opened is $O(\log n)$.}
 
 \pagebreak
 
 \item \emph{Closest Points:} Imagine $n$ points are distributed uniformly at random on a circle with circumference $1$.  Show that the expected number of pairs of points that are within distance $\theta(1/n^{2)}$ of each other is greater than $1$ (note that this implies that the smallest distance between two points is likely $O(1/n^{2})$).  Hint: Partition the circle into $n^{2}/k$ regions of size $k/n^{2}$ for some constant $k$; then use the Birthday paradox to solve for the necessary $k$.
 \ans{Think of the regions as bins and the points as balls.  The probability that two points fall in the same region is $k/n^{2}$.   Let $X_{i,j}$ equal $1$ if points $i$ and $j$ fall in the same region and $0$ otherwise and let $X$ be the sum over all $n \choose 2$ values $i$ and $j$ of $X_{i,j}$.  Using linearity of expectation, $E(X) = {n \choose 2}k/n^{2}$.  Thus, for some $k$, e.g. $k\geq 3$, we would expect at least one pair of balls to fall in the same bin.}

\end{itemize}


 \problem{Are you Smarter than a 561 Student?}
 
You are competing in the popular game show ``Let's Make a Dynamic Program'' with another player.  You and your opponent both start with $0$ dollars.  If you reach (or exceed) $n$ dollars before your opponent, you win $n$ dollars; if your opponent reaches (or exceeds) $n$ dollars before you, you win nothing; and if you both reach (or exceed) $n$ dollars at the same time, you both win $n$ dollars.  In each turn, you get to choose the level of difficulty of the next question asked, where this difficulty is represented by an integer $k$ between $1$ and $n$.  If you answer the question correctly, you get $k$ dollars, otherwise your opponent gets $k$ dollars.  Note that you are always in control throughout the entire game of the difficulty level of the question asked.
 
Through careful study of the game you have been able to determine the probability $p_{i}$ for $i$ between $1$ and $n$, which is the probability that you will answer a question of difficulty $i$ correctly.

\begin{itemize}

\item Consider the greedy algorithm where you always choose a question of difficulty level $i$ for $i$ maximizing $i*p_{i} - i*(1-p_{i}) = i(2p_{i}-1) $.  Is this an optimal algorithm?  Hint: Is it ever better to make a long shot bet because the probability of success from multiple short bets is small.  In particular,think about the case where your opponent has $n-1$ dollars and you have $0$.


\ans{Greedy is not optimal.  Consider the case where $p_{1} = 1/4$, $p_{n} = 1/5$ and for all other $i$, $p_{i} = 0$.  Then $1$ is the difficulty level maximizing the expected difference between you and your opponent, since $1p_{1} - 1q_{1} = -1/2$ and $np_{n} - nq_{n} = -(3/5)n$.  Assume further that your opponent has $n-1$ dollars and you have $0$ dollars.   If you choose a difficulty of $n$, you have the chance of jumping ahead of your opponent to victory, which happens with probability at least $1/5$.  In contrast, the probability of success by following the greedy strategy, where you just keep choosing difficulty of $1$ will only be $(1/4)^{n}$, which is exponentially small in $n$.}

\pagebreak

\item Let state $(i,j)$ be the state where you have $i$ dollars and your opponent has $j$ dollars.  Note that if you choose the difficulty level to be $k$ at that state, you have probability $p_{k}$ of going to state $(i+k,j)$ and probability $(1-p_{k})$ of going to state $(i,k+jj)$.  Now let $e(i,j)$ be your expected winnings if you have $i$ dollars and your opponent has $j$ dollars and you play optimally.  Write a recurrence relation for the value $e(i,j)$.  Note: You will find it useful to consider $i$ and $j$ values that range from $0$ to $2n-1$.  Hint: Use expected values for simpler subproblems and the probabilities $p_{i}$ described above to compute $e(i,j)$.  Don't forget the base case(s).

\ans{Base Cases: $e(i,j) = n$ for all $j \geq n$.  $e(i,j) = 0$ for all $i \geq n$ and $j < n$.  \\ Recurrence:  for other values of $i$ and $j$,
 $e(i,j) = max_{1 \leq k \leq n} p_{k} e(i+k,j) + (1-p_{k}) e(i,j+k)$}

\pagebreak


\item Give the pseudocode for computing the value $e(0,0)$, which gives you your expected winnings if you play this game optimally.
\ans{Basically, need to fill in the base case first of a n by n array and then fill in the remaining values from bottom up and right to left}
  
\pagebreak 

\item What if the game is changed as follows.  You still select the difficulty level $k$, but after your selection, both you and your opponent have the chance to write down the answer to the question.  Whoever gets the answer correct wins $k$ dollars (note that both of you may win now).  There is no penalty for a wrong answer.  The probability that you answer a question of difficulty $k$ correctly is $p_{k}$ and the probability that your opponent answers correctly is $q_{k}$.   Can you still solve this new problem using dynamic programming?  If so, give a recurrence and describe how to change the algorithm.  If not, describe why not.

\ans{It is still possible.  The base case for the recurrence is the same as before.  The new part of the recurrence is determined by following equation for fixed $k$:
$e(i,j) =  (p_{k}q_{k} e(i+k,j+k) + (1-p_{k})q_{k}e(i,j+k) + p_{k}(1-q_{k})e(i+k,j) + (1-p_{k})(1-q_{k}) e(i,j)$.  Note that $e(i,j)$ appears on both the left and right side of this equation, which at first seems to cause problems in formulating a recurrence; however, it is still possible to isolate $e(i,j)$ on the left side.  Then it is possible to get a proper recurrence by taking the maximum over all possible values of $k$.  The recurrence and pseudocode is left as an exercise.}
  
 \end{itemize}
  
  \end{enumerate}
  

 
 
 \problem{Borges Library - determining if all the books in a given room are the same}
1) Create a Bloom Filter for all the books in the room
 
2) How to compare two rooms quickly to determine if they have the same set of books or not.


 
 
\problem{Data Structure Problem (BST, skip list, etc, heap)}
 
 
 
 \problem{Bday}
  
 \problem{Hafts}
 
 
 \problem{Skip Lists}
 
 \begin{itemize}
 \item How would you merge two skip lists?
 \item How would you find the $i$-th element in sorted order
 
\end{itemize}

 
 
\problem{Search Trees}

Consider a tree with the following properties:

\begin{itemize}
\item Each internal node has exactly three children
\item The heights of the subtrees rooted at each child differ by at most $1$.
\end{itemize}

What is the maximum height of such a tree containing $n$ nodes?

Hint: Write a recurrence relation for the maximum number of nodes as a function of the height and then solve for the height.  Show your work!

\ans{Let $T(h)$ be the maximum number of nodes in a tree of height $h$.  Then $T(h) = T(h-1) + 2 T(h-2) + 1$.  $(\Le^{2}-\Le - 2) = (\Le -2)(\Le +1)$ annihilates the homogeneous part and $\Le - 1$ annihilates the non-homogeneous part.  Thus, the solution is of the form $T(h) = c_{1}2^{h} + c_{2} + c_{3} (-1)^{h}$.  Let $n$ be the number of nodes in the tree.  We know that $n \geq T(h)$ and so $n \geq c_{1}2^{h} + c_{2} + c_{3} (-1)^{h}$.  If we let $c = c_{2} - c_{3}$, we can say that, $n \geq c_{1} 2^{h} + c$.  Taking logs of both sides, we have that $\log n \geq h \log (2c_{1}) + \log c$.  This implies that $h \leq 1/(log(2c_{1}))(\log n - \log c)$.  The right hand side is $O(\log n)$.  Thus, $h$ is $O(\log n)$.}

\problem{Hash Tables and Probability}

Assume we hash $n$ items into a hash table with $n$ bins using a good hash function i.e. each item is hashed to a bin chosen independently and uniformly at random.  Give a good upper bound on the number of empty bins.  Solve for the constants in your upper bound i.e. do not use asymptotic notation.

Hint: Use the fact that $1-x \leq e^{-x}$ for all $x$.

\ans{Let $X_{i}$ be an indicator random variable which is $1$ if the $i$-th bin is empty and is $0$ otherwise.   Then note that $E(X_{i})$ equals the probability that the $i$-th bin is empty.  This is the probability that the $n$ items do not fall in bin $i$.  The probability that a single item does not fall in bin $i$ is exactly $1 - 1/n$.  Since the items are all hashed independently, the probability that \emph{no} item hashes into bin $i$ is exactly $(1-1/n)^{n}$.  Using the hint, note that  $(1-1/n)^{n} \leq (e^{-1/n})^{n} = e^{-1}.$  Let $X$ be a random variable giving the number of empty bins and note that $X = \sum_{i=1}^{n} X_{i}$.  Using linearity of expectation, we see that $E(X) = E(\sum_{i=1}^{n} X_{i}) = \sum_{i=1}^{n} E(X_{i}) \leq n/e$.  Thus the expected expected number of empty bins is at most $n/e$.  This is actually a pretty tight upper bound as $n$ gets large.}

\problem{Divide and Conquer}

Imagine that after graduating from UNM, you start your new job at the exciting investment banking firm SELLOUT, Inc.  The firm if faced with the following problem: they have an array of the predicted prices of a stock over $n$ days and they want to determine, using this array, exactly one day to buy the stock and one day to sell the stock in order to maximize their profit.

The problem can be formally stated as follows.  You are given an array $A$ of $n$ numbers.  You want to choose indices $1 \leq i < j \leq n$ such that $A[j] - A[i]$ is maximized over all such indices.  Give an $o(n^{2})$ algorithm to solve this problem.

\ans{Use Recursion!  Recursively find the pair of indices $i_{l}$ and $j_{l}$ on the left half of the array such that $i_{l} < j_{j}$ and $A[j]-A[i]$ is maximized over all such pairs.  Find a similar pair $i_{r}$ and $j_{r}$ on the right half of the array.  Now find $x$, the index of the element with smallest value on the left half and $y$, the index of the largest element on the right half.  Finally, return 
max($A[j_{l}] - A[i_{l}]$, $A[j_{r}] - A[i_{r}]$, $A[y] - A[x]$).  The run time of this algorithm is given by the same recurrence as for merge sort $T(n) = 2T(n/2) + n$, whose solution is $n \log n$.  Note that we can get an even faster algorithm than this using dynamic programming. Also there is another $O(n \log n)$ solution that makes use of a heap or BST.}

\end{enumerate}

\end{document}





% \item {\bf True or False}:  In a max-heap, the element with smallest key is always at the
% rightmost leaf node of the heap? \ans{F: it's always at a leaf node
% but not necessarily the rightmost leaf node}
% \item {\bf True or False}:  Bubblesort requires $O (n)$ extra space (not counting the space
% to store the array to be sorted)? \ans{F: it takes $O (1)$ extra space}
% \item {\bf True or False}:  $1/\log n$ is $o (1)$? \ans{T}
% \item {\bf True or False}:  $\log \sqrt{n}$ is $\Theta (\log n)$?
% \ans{T: since $\log \sqrt{n} = 1/2\log n$}
% \item {\bf True or False}:  $2^{n-1}$ is $o (2^{n})$?\ans{F: since $2^{n-1}=1/2*2^{n}$}
% \problem{Annihilators}\\

% Consider the recurrence $T (n) = 2T (n-1) - T (n-2) + 4$, $T (0)=0$, $T (1)=0$.
% Solve this recurrence \emph{exactly} using annihilators.  Don't forget
% to check your answer.

% \ans{Consider the homogeneous part first.  Let $T_{n} = 2T (n-1) - T
% (n-2)$, and $T = \seqi{T_{n}}$.  Then
% \begin{eqnarray}
% T & = & \seqi{T_{n}}\\
% \Le T & = & \seqi{T_{n+1}}\\
% \Le^{2} T & = & \seqi{T_{n+2}}
% \end{eqnarray}
% Since $\seqi{T_{n+2}} = \seqi{2T_{n+1}-T_{n}}$, we know that
% $\Le^{2}T-2\Le T + T = \seqi{0}$, and thus $\Le^{2}-2\Le +1 = (\Le -1)
% (\Le -1)$ annihilates $T$.  Further we know that $(\Le -1)$
% annihilates the non-homogeneous part.  Thus the annihilator of the
% whole sequence is $(\Le -1)^{3}$.  Thus $T (n)$ is of the form:
% \[
% T (n) = c_{1}n^{2}+c_{2}n+c_{3}
% \]
% We know:
% \begin{eqnarray}
% T (0) = 0 & = & c_{3}\\
% T (1) = 0 & = & c_{1} + c_{2}\\
% T (2) = 4 & = & 4 c_{1} + 2c_{2}
% \end{eqnarray}
% so $c_{1}=2$, $c_{2}=-2$, $c_{3}=0$ and thus
% \[
% T (n) = 2n^{2}-2n
% \]
% Check: $T (3) = 2*4-0+4=12$ and $2*9-6=12$.}




% \problem{Substitution Method}\\
% Consider the following recurrence: $$T(n) = T(n-1) + T(n-2) - n + 3,$$
% where $T(1) = 1,T(2)=2$.\\ \ \\ Show that $T(n) = n$ by induction.
% Include the following in your proof: 1)the base case(s) 2)the
% inductive hypothesis and 3)the inductive step.

% \ans{Base Case: T(1) = 1.\\Inductive Hypothesis: For all $j<n$, T(j) =
% j\\Inductive Step: We must show that $T(n)=n$, assuming the inductive
% hypothesis.  
% \begin{eqnarray}
% T(n) & = & T(n-1) + T(n-2) - n + 3\\
% T(n) & = & (n-1) + (n-2) - n + 3\\
% T(n) & = & n
% \end{eqnarray}
% where the inductive hypothesis allows us to make the replacements in
% the second step.}

% \item {\bf True or False}: If $X$ and $Y$ are sequences that both
% begin with the character $a$, then some longest common subsequence of
% $X$ and $Y$ begins with the character $a$. \ans{True}

 \problem{Data Structures}
 
 \begin{enumerate}
 
Your colleague wants to change the rules of red-black trees to the following:
 \begin{itemize}
\item The root node and leaf nodes (NIL) can be either red or black
\item If a node is red and not a leaf node, both of its children are black
\item If a node is black and not a leaf node, both of its children are red
\item For each node, all paths from the node to descendant leaves contain the same number of black nodes
\end{itemize}
\item (6 points) Is it possible to use these rules to create a balanced BST data structure?  If so, sketch your solution.  If not, show how things can go wrong with a minimum size counter-example.
 
 \ans{This fails.  Consider a tree of size $n=4$.  There is no way to color it to satisfy the first three rules without violating the fourth one.}
 
 \pagebreak

\item (5 points) Your boss wants to create the following data structure in the comparison model and to name it after himself, the \emph{Merkle}.  A Merkle has the following operations and properties on it.  BuildMerkle takes an arbitrary array and builds a Merkle from it in $O(n)$ time.  The resulting Merkle will provide the following operations.  FindMin (resp. FindMax) will return the minimum (resp. maximum) element and run in $O(\log n)$ time.  Successor(x) (resp. Predecessor(x)) return the next largest (resp. smallest) element in the Merkle after the element $x$, and both of these operations run in $O(1)$ time.  Intuitively, your boss wants you to combine the nice properties of the heap with the nice properties of a data structure like skip lists.  Can you immortalize your boss's name in CS textbooks by creating this data structure?

\ans{No.  This would allow sorting in $O(n)$ time.  To show this, first build the Merkle from an unsorted array, then find the minimum element and keep calling successor}



\pagebreak

In this problem, you will modify count-min sketches so that they handle negative counts.  As in class, assume you are presented with a stream of tuples of the form $(i_{t},c_{t})$, except now $c_{t}$ may be either a negative or positive integer.  The data structure you will use will consist of two count-min sketches, a positive count-min sketch for positive counts and a negative count-min sketch for negative counts.  In particular, each of the two sketches will use $m$ counters and $k$ hash functions, where all hash functions can be assumed to be independent.  If $c_{t}$ is positive, in the positive count-min sketch (positive sketch for short),  for each $1 \leq a \leq k$, $C_{a,h_{a}(i)}$ will be incremented by $c_{t}$.  If $c_{t}$ is negative, in the negative sketch, for each $1 \leq a \leq k$, $C_{a,h_{a}(i)}$ will be incremented by $-c_{t}$.   The estimate of the count of an item, $i$ at time $T$ is $m^{+}(i,T) - m^{-}(i,T$, where $m^{+}(i,T)$ is the value of the smallest counter associated with $i$ in the positive sketch and $m^{-}(i,T)$ is the value of the smallest counter associated with $i$ in the negative sketch.  As in class, let $Count(i,T)$ be the true count of item $i$ in the stream up to time $T$.  Also assume that $k = m \epsilon/e$ for the positive sketch and for the negative sketch.
 
 \item (7 points) Give a good bound on the probability that the following holds:\\
 $$ Count(i,T) - \epsilon \sum_{i=1}^{T} |c_{i}| \leq m(i,T) \leq Count(i,T) + \epsilon \sum_{i=1}^{T} |c_{i}| $$
 Please prove your bound.
 
 
 \ans{Let $S^{+}_{T}$ be the sum of all the positive counts in the stream up to time $T$.  For the positive sketch, we know that $Pr(Z_{j}> \epsilon S^{+}_{T}) \leq e^{m\epsilon/e}$, where $Z_{j}$ is the amount the min counter associated with $i$ in the positive sketch is incremented by items other than $i$.  Let $S^{-}_{T}$ be the sum of the absolute values of the negative counts in the stream up to time $T$.  Then we also know using the analysis in class that for the negative sketch, $Pr(Z'_{j}> \epsilon S^{-}_{T}) \leq e^{m\epsilon/e}$, where $Z'_{j}$ is the amount the min counter associated with $i$ in the negative sketch is incremented by items other than $i$.   A simple union bound on these two inequalities shows that $Pr(Z_{j}> \epsilon S^{+}_{T} \textrm{ or } Z'_{j}> \epsilon S^{-}_{T}) \leq 2 e^{m\epsilon/e}$.  Thus, we can see that $Pr(Z_{j} + Z'_{j} > \epsilon \sum_{i=1}^{T} |c_{i}|) \leq  2 e^{m\epsilon/e}$.  Thus the probability the approximation bound given holds is at least $1 - 2 e^{m\epsilon/e}$. }

\pagebreak

\item  (7 points) Now imagine you are given a constant number of data streams $D_{1}, D_{2}, \ldots, D_{c}$ and weights associated with them $w_{1}, w_{2}, \ldots w_{c}$ that may be positive or negative real numbers.  For each item $i$, at time $T$, define $Count(i,T)$ to be the weighted sum of the count values seen in all data streams up to time $T$, where a count value seen in stream $i$ is weighted by $w_{i}$.  Assume now that all count values seen are positive.  Describe a data structure based on count-min sketches that can approximate $Count(i,T)$.  How much memory does your data structure use? How closely can you approximate $Count(i,T)$ and with what probability?  Please justify your answers.  For consistency in notation, please let $S(i,j,T)$ be the sum of the counts of item $i$ in stream $j$ up to time $T$.

\ans{The basic idea is to use $c$ different count min-sketches and let $m(i,T)$ be the weighted sum of the min value in each of them associated with the item $i$.  The total memory used is $cm$.  A union bound over the $c$ different data streams can establish the following guarantee with probability $1-c e^{m\epsilon/e}$:
 $$ Count(i,T) - \epsilon \sum_{j=1}^{c} |w_{j}| S(i,j,T) \leq m(i,T) \leq Count(i,T) + \epsilon \sum_{j=1}^{c} |w_{j}| S(i,j,T) $$}


\end{enumerate}
 
 
 
 \problem{Dynamic Programming}

Consider a collection of $n$ nodes aligned on a line, numbered $1$ to $n$.  Two nodes are connected by an edge if they are adjacent on the line, e.g. nodes $i$ and $i+1$ are neighbors.  For each pair $i$, $i+1$ of neighboring nodes, there is a weight $w_{i,i+1}$ associated with the pair, which may be either positive or negative.

In this problem, each node will be colored with one of two colors, red or blue.  If a pair $(i,i+1)$ of neighboring nodes are colored the same, the cost associated with that pair is $w_{i,i+1}$; if the pair are colored differently, the cost associated with that pair is $-w_{i,i+1}$.   The total cost of a coloring is the sum of the costs of all neighboring pairs.

\begin{enumerate}

\item (10 points) Describe a dynamic program to output the minimum cost of any coloring, when given all edge weights.  Hint: Let $c(i,r)$ be the minimum cost of coloring nodes $1$ through $i$ when node $i$ is colored red.  Let $c(i,b)$ be the minimum cost of coloring nodes $1$ through $i$ when node $i$ is colored blue.

\ans{The recurrence relation is the following:\\
$c(1,r) = c(1,b) = 0$\\
$c(i,b) = \textrm{min } (c(i-1, r) - w(i-1,i), c(i-1, b) + w(i-1,i))$\\
$c(i,r) = \textrm{min } (c(i-1, r) + w_(i-1,i), c(i-1, b) - w(i-1,i))$
The dynamic program just keeps an array of size $n$ with all these values and fills it in from left to right.  The final value returned is the minimum of $c(n,b)$ and $c(n,r)$.
}



\pagebreak

Now imagine that the nodes are connected in a $n$ by $n$ grid, and that each node can be colored with $m$ possible colors.  There is an edge between a pair of nodes on the grid if they are immediately adjacent either horizontally or vertically; again, each edge has a weight associated with it that may be either positive or negative.  (For example neighboring nodes $(i,j)$ and $(i+1,j)$ would have an edge with weight $w((i,j),(i+1,j))$) 

\item (15 points) Describe a dynamic program to output the minimum cost of any coloring, when given all edge weights for a grid.  What is the runtime of your algorithm?

%\footnote{One motivation for this problem could be finding the best k clustering of people in a grid-like social network when given information about the strength of positive and negative social interactions (these would correspond to the edge weights) and the colors would correspond to clusters.}

\ans{Let $c(i,j,k)$ be the minimum cost of coloring all nodes in rows above $i$ and all nodes in row $i$ up to node $(i,j)$, when node $i,j$ is colored with color $k$.  Let $S(k,k')$ be $1$ if $k=k'$ and $-1$ otherwise.   For convenience, we'll assume zero edge weights for edges that don't exist in the grid.
The recurrence relation is the following:\\
$c(1,1,x) = 0$ for all $x$ between $1$ and $m$\\
$c(i,0,x) = 0$ for all $x$ between $1$ and $m$ and all $i$ between $1$ and $n$\\
$c(0,j,x) = 0$ for all $x$ between $1$ and $m$ and all $j$ between $1$ and $n$\\
$c(i,j,k) = \textrm{min}_{k'} (c(i-1,j,k') + w((i-1,j),(i,j)) * S(k,k')\\ + \textrm{min}_{k'} (c(i,j-1,k') + w((i,j-1),(i,j)) * S(k,k')$\\
The dynamic program fills in a $n$ by $n$ by $m$ dimensional array.  The variables $i$ and $j$ move left right and top down.  For each fixed $i$ and $j$ value, all $k$ values between $1$ and $m$ are filled in, before proceeding to the next $i,j$ value.  The final value returned is the $c(n,n,k)$ value that is smallest over all valid $k$.}
  
 
 \end{enumerate}



\problem{Asymptotic Notation} \newline

Prove that $\sqrt{n} = O(\frac{n}{\log n})$.  

\ans{Goal: Give $c$ and $n_{0}$ such that $\sqrt{n} \leq
c\frac{n}{\log n}$ for all $n\geq n_{0}$.  The inequality we want then is:
\begin{eqnarray}
\sqrt{n} & \leq & c\frac{n}{\log n}\\
\sqrt{n} * \frac{\log n}{n} & \leq & c\\
\frac{\log n}{\sqrt{n}} & \leq & c\\
\end{eqnarray}
So for $c=1$ and $n_{0}=1$, it's the case that $\sqrt{n} \leq
c\frac{n}{\log n}$ for all $n\geq n_{0}$
}
